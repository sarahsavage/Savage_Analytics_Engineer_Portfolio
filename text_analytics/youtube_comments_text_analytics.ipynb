{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c536087e-6755-47ce-9207-83efacf7aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. import relevant libraries\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bd55aa9-65d2-4ebe-9e39-c8daaa6fb71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment      object\n",
      "Sentiment    object\n",
      "dtype: object\n",
      "                                             Comment Sentiment\n",
      "0  lets not forget that apple pay in 2014 require...   neutral\n",
      "1  here in nz 50 of retailers don’t even have con...  negative\n",
      "2  i will forever acknowledge this channel with t...  positive\n",
      "3  whenever i go to a place that doesn’t take app...  negative\n",
      "4  apple pay is so convenient secure and easy to ...  positive\n"
     ]
    }
   ],
   "source": [
    "## 2. Create a dataframe from table\n",
    "\n",
    "df = pd.read_csv('YoutubeCommentsDataSet.csv')\n",
    "\n",
    "# Confirm data types\n",
    "print(df.dtypes)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b4ce77-7a1f-4040-97d5-8b30b62a4248",
   "metadata": {},
   "source": [
    "This script was originally developed to provide insights into the free-text fields in user surveys. I wrote it intending to be a \"plug and play\" script that could be used for any text-based data set with minimal editing. This dataset was downloaded from Kaggle and already has sentiment tags. I'm curious to see whether my sentiment analysis script matches the original. Using pre-trained cloud models saves on storage and training time, but another option would be to train and store our own model. For the original work, we've decided Bert is good enough and worth the tradeoff. If there were additional text-based fields, steps 3-5 would be repeated for each. In this case, there is only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2055827-696a-49e7-a545-ad564e734083",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets not forget that apple pay in 2014 require...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>here in nz 50 of retailers don’t even have con...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i will forever acknowledge this channel with t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whenever i go to a place that doesn’t take app...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay is so convenient secure and easy to ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18403</th>\n",
       "      <td>i really like the point about engineering tool...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18404</th>\n",
       "      <td>i’ve just started exploring this field and thi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18405</th>\n",
       "      <td>excelente video con una pregunta filosófica pr...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18406</th>\n",
       "      <td>hey daniel just discovered your channel a coup...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18407</th>\n",
       "      <td>this is great focus is key a playful approach ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18364 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Comment Sentiment\n",
       "0      lets not forget that apple pay in 2014 require...   neutral\n",
       "1      here in nz 50 of retailers don’t even have con...  negative\n",
       "2      i will forever acknowledge this channel with t...  positive\n",
       "3      whenever i go to a place that doesn’t take app...  negative\n",
       "4      apple pay is so convenient secure and easy to ...  positive\n",
       "...                                                  ...       ...\n",
       "18403  i really like the point about engineering tool...  positive\n",
       "18404  i’ve just started exploring this field and thi...  positive\n",
       "18405  excelente video con una pregunta filosófica pr...   neutral\n",
       "18406  hey daniel just discovered your channel a coup...  positive\n",
       "18407  this is great focus is key a playful approach ...  positive\n",
       "\n",
       "[18364 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3.remove Nan fields to exclude from topic creation\n",
    "##This step is unnecessary in the current dataset, which has been pre-cleaned but it won't always be clean data so good to keep it in\n",
    "df_clean = df[['Comment', 'Sentiment']].dropna()\n",
    "\n",
    "#cast text column as string to ensure that there aren't a few random floats or emojis thrown into the mix\n",
    "df_clean['Comment'] = df_clean['Comment'].astype(str)\n",
    "\n",
    "##check filtered dataframe\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f203b86-15eb-44ad-b07a-9e5f332f00c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Convert to list for vectorization\n",
    "docs = df_clean['Comment'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53f3091b-a409-44d4-86ec-e7b609aadb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n",
      "2025-04-09 11:12:40,894 - BERTopic - Embedding - Transforming documents to embeddings.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c56abf77d2b4afa8ced6f53d9832a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/574 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 11:13:47,946 - BERTopic - Embedding - Completed ✓\n",
      "2025-04-09 11:13:47,951 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2025-04-09 11:14:02,056 - BERTopic - Dimensionality - Completed ✓\n",
      "2025-04-09 11:14:02,060 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "2025-04-09 11:14:05,333 - BERTopic - Cluster - Completed ✓\n",
      "2025-04-09 11:14:05,344 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2025-04-09 11:14:05,622 - BERTopic - Representation - Completed ✓\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>full_comment_text</th>\n",
       "      <th>sentiment_original</th>\n",
       "      <th>topic_number</th>\n",
       "      <th>sentiment_new</th>\n",
       "      <th>sentiment_score_new</th>\n",
       "      <th>topic_count</th>\n",
       "      <th>topic_name</th>\n",
       "      <th>representation</th>\n",
       "      <th>docs</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lets not forget that apple pay in 2014 require...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>9</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.556387</td>\n",
       "      <td>381.0</td>\n",
       "      <td>9_apple_iphone_pro</td>\n",
       "      <td>[apple, iphone, pro]</td>\n",
       "      <td>[order of event prediction 1 intro what apple ...</td>\n",
       "      <td>[(shes, 0.058790930161863104), (makeup, 0.0385...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>here in nz 50 of retailers don’t even have con...</td>\n",
       "      <td>negative</td>\n",
       "      <td>9</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.770144</td>\n",
       "      <td>381.0</td>\n",
       "      <td>9_apple_iphone_pro</td>\n",
       "      <td>[apple, iphone, pro]</td>\n",
       "      <td>[order of event prediction 1 intro what apple ...</td>\n",
       "      <td>[(shes, 0.058790930161863104), (makeup, 0.0385...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>whenever i go to a place that doesn’t take app...</td>\n",
       "      <td>negative</td>\n",
       "      <td>9</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.722036</td>\n",
       "      <td>381.0</td>\n",
       "      <td>9_apple_iphone_pro</td>\n",
       "      <td>[apple, iphone, pro]</td>\n",
       "      <td>[order of event prediction 1 intro what apple ...</td>\n",
       "      <td>[(shes, 0.058790930161863104), (makeup, 0.0385...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>apple pay is so convenient secure and easy to ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>9</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.946898</td>\n",
       "      <td>381.0</td>\n",
       "      <td>9_apple_iphone_pro</td>\n",
       "      <td>[apple, iphone, pro]</td>\n",
       "      <td>[order of event prediction 1 intro what apple ...</td>\n",
       "      <td>[(shes, 0.058790930161863104), (makeup, 0.0385...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>we’ve been hounding my bank to adopt apple pay...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>9</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.477529</td>\n",
       "      <td>381.0</td>\n",
       "      <td>9_apple_iphone_pro</td>\n",
       "      <td>[apple, iphone, pro]</td>\n",
       "      <td>[order of event prediction 1 intro what apple ...</td>\n",
       "      <td>[(shes, 0.058790930161863104), (makeup, 0.0385...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18350</th>\n",
       "      <td>awesome succinct very effective for a newbie t...</td>\n",
       "      <td>positive</td>\n",
       "      <td>14</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.960398</td>\n",
       "      <td>253.0</td>\n",
       "      <td>14_tutorial_thank_explained</td>\n",
       "      <td>[tutorial, thank, explained]</td>\n",
       "      <td>[i still want to see that tutorial, i could cr...</td>\n",
       "      <td>[(data, 0.14780327371328625), (science, 0.0753...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18351</th>\n",
       "      <td>well explanation love it</td>\n",
       "      <td>positive</td>\n",
       "      <td>14</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.890718</td>\n",
       "      <td>253.0</td>\n",
       "      <td>14_tutorial_thank_explained</td>\n",
       "      <td>[tutorial, thank, explained]</td>\n",
       "      <td>[i still want to see that tutorial, i could cr...</td>\n",
       "      <td>[(data, 0.14780327371328625), (science, 0.0753...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18353</th>\n",
       "      <td>thanks for sharing what is machine learning us...</td>\n",
       "      <td>positive</td>\n",
       "      <td>26</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.966756</td>\n",
       "      <td>145.0</td>\n",
       "      <td>26_learning_machine_ml</td>\n",
       "      <td>[learning, machine, ml]</td>\n",
       "      <td>[please make another more videos on machine le...</td>\n",
       "      <td>[(life, 0.04247010384916758), (reading, 0.0358...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18354</th>\n",
       "      <td>i am currently enrolled in a msc machine learn...</td>\n",
       "      <td>positive</td>\n",
       "      <td>26</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.689571</td>\n",
       "      <td>145.0</td>\n",
       "      <td>26_learning_machine_ml</td>\n",
       "      <td>[learning, machine, ml]</td>\n",
       "      <td>[please make another more videos on machine le...</td>\n",
       "      <td>[(life, 0.04247010384916758), (reading, 0.0358...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18361</th>\n",
       "      <td>excelente video con una pregunta filosófica pr...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.757643</td>\n",
       "      <td>691.0</td>\n",
       "      <td>2_que_el_como</td>\n",
       "      <td>[que, el, como]</td>\n",
       "      <td>[me estoy aprovechando de ti esta noche cazart...</td>\n",
       "      <td>[(song, 0.08805806160025687), (music, 0.059501...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12347 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       full_comment_text sentiment_original  \\\n",
       "0      lets not forget that apple pay in 2014 require...            neutral   \n",
       "1      here in nz 50 of retailers don’t even have con...           negative   \n",
       "3      whenever i go to a place that doesn’t take app...           negative   \n",
       "4      apple pay is so convenient secure and easy to ...           positive   \n",
       "5      we’ve been hounding my bank to adopt apple pay...            neutral   \n",
       "...                                                  ...                ...   \n",
       "18350  awesome succinct very effective for a newbie t...           positive   \n",
       "18351                           well explanation love it           positive   \n",
       "18353  thanks for sharing what is machine learning us...           positive   \n",
       "18354  i am currently enrolled in a msc machine learn...           positive   \n",
       "18361  excelente video con una pregunta filosófica pr...            neutral   \n",
       "\n",
       "       topic_number sentiment_new  sentiment_score_new  topic_count  \\\n",
       "0                 9       neutral             0.556387        381.0   \n",
       "1                 9      negative             0.770144        381.0   \n",
       "3                 9      negative             0.722036        381.0   \n",
       "4                 9      positive             0.946898        381.0   \n",
       "5                 9       neutral             0.477529        381.0   \n",
       "...             ...           ...                  ...          ...   \n",
       "18350            14      positive             0.960398        253.0   \n",
       "18351            14      positive             0.890718        253.0   \n",
       "18353            26      positive             0.966756        145.0   \n",
       "18354            26      positive             0.689571        145.0   \n",
       "18361             2       neutral             0.757643        691.0   \n",
       "\n",
       "                        topic_name                representation  \\\n",
       "0               9_apple_iphone_pro          [apple, iphone, pro]   \n",
       "1               9_apple_iphone_pro          [apple, iphone, pro]   \n",
       "3               9_apple_iphone_pro          [apple, iphone, pro]   \n",
       "4               9_apple_iphone_pro          [apple, iphone, pro]   \n",
       "5               9_apple_iphone_pro          [apple, iphone, pro]   \n",
       "...                            ...                           ...   \n",
       "18350  14_tutorial_thank_explained  [tutorial, thank, explained]   \n",
       "18351  14_tutorial_thank_explained  [tutorial, thank, explained]   \n",
       "18353       26_learning_machine_ml       [learning, machine, ml]   \n",
       "18354       26_learning_machine_ml       [learning, machine, ml]   \n",
       "18361                2_que_el_como               [que, el, como]   \n",
       "\n",
       "                                                    docs  \\\n",
       "0      [order of event prediction 1 intro what apple ...   \n",
       "1      [order of event prediction 1 intro what apple ...   \n",
       "3      [order of event prediction 1 intro what apple ...   \n",
       "4      [order of event prediction 1 intro what apple ...   \n",
       "5      [order of event prediction 1 intro what apple ...   \n",
       "...                                                  ...   \n",
       "18350  [i still want to see that tutorial, i could cr...   \n",
       "18351  [i still want to see that tutorial, i could cr...   \n",
       "18353  [please make another more videos on machine le...   \n",
       "18354  [please make another more videos on machine le...   \n",
       "18361  [me estoy aprovechando de ti esta noche cazart...   \n",
       "\n",
       "                                                   words  \n",
       "0      [(shes, 0.058790930161863104), (makeup, 0.0385...  \n",
       "1      [(shes, 0.058790930161863104), (makeup, 0.0385...  \n",
       "3      [(shes, 0.058790930161863104), (makeup, 0.0385...  \n",
       "4      [(shes, 0.058790930161863104), (makeup, 0.0385...  \n",
       "5      [(shes, 0.058790930161863104), (makeup, 0.0385...  \n",
       "...                                                  ...  \n",
       "18350  [(data, 0.14780327371328625), (science, 0.0753...  \n",
       "18351  [(data, 0.14780327371328625), (science, 0.0753...  \n",
       "18353  [(life, 0.04247010384916758), (reading, 0.0358...  \n",
       "18354  [(life, 0.04247010384916758), (reading, 0.0358...  \n",
       "18361  [(song, 0.08805806160025687), (music, 0.059501...  \n",
       "\n",
       "[12347 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Initialize BERTopic Model\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "topic_model = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    calculate_probabilities=True,\n",
    "    min_topic_size=50,\n",
    "    verbose=True,\n",
    "    n_gram_range=(1, 2),\n",
    "    top_n_words=3\n",
    ")\n",
    "\n",
    "# Initialize Sentiment Analysis Model\n",
    "sentiment_classifier = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model='cardiffnlp/twitter-roberta-base-sentiment-latest',\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Perform topic classification and sentiment analysis in one pass\n",
    "topics, probabilities = topic_model.fit_transform(docs)\n",
    "sentiment_results = sentiment_classifier(docs)\n",
    "\n",
    "# Collect data for the final DataFrame\n",
    "topic_numbers = topics  # Topic classification results\n",
    "sentiment_labels = [res['label'] for res in sentiment_results]\n",
    "sentiment_scores = [res['score'] for res in sentiment_results]\n",
    "\n",
    "# Create DataFrame for topic information\n",
    "df_topics = pd.DataFrame({'Topic': list(topic_model.get_topics().keys()),\n",
    "                          'topic_words': list(topic_model.get_topics().values())})\n",
    "df_topic_info = pd.DataFrame(topic_model.get_topic_info()).iloc[1:].reset_index(drop=True)\n",
    "df_topic_info['topic_words'] = df_topics['topic_words']\n",
    "\n",
    "# Add topic and sentiment results to the original DataFrame\n",
    "df_clean['Topic'] = topic_numbers\n",
    "df_clean['sentiment_new'] = sentiment_labels\n",
    "df_clean['sentiment_score_new'] = sentiment_scores\n",
    "\n",
    "# Merge with topic information\n",
    "df_topic = df_clean.merge(df_topic_info, how='left', on='Topic')\n",
    "\n",
    "# Drop coordinates if they exist\n",
    "columns_to_drop = ['x_coordinate', 'y_coordinate']\n",
    "df_topic = df_topic.drop(columns=[col for col in columns_to_drop if col in df_topic.columns], axis=1)\n",
    "\n",
    "# Filter out outlier topics (-1)\n",
    "df_topic = df_topic[df_topic['Topic'] != -1]\n",
    "\n",
    "# Optional: Filter by topic size\n",
    "min_topic_size = 50\n",
    "##this number should be adjusted as the size of the dataset increases\n",
    "large_topics = df_topic_info[df_topic_info['Count'] >= min_topic_size]['Topic']\n",
    "df = df_topic[df_topic['Topic'].isin(large_topics)]\n",
    "\n",
    "# Rename columns for clarity\n",
    "##in the case of multiple text columns, a column name signifier would also be added\n",
    "df.rename(columns={\n",
    "    'Comment': 'full_comment_text',\n",
    "    'Sentiment': 'sentiment_original',\n",
    "    'Topic': 'topic_number',\n",
    "    'Name': 'topic_name',\n",
    "    'Representation': 'representation',\n",
    "    'Representative_Docs': 'docs',\n",
    "    'topic_words': 'words',\n",
    "    'Count': 'topic_count'\n",
    "}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Final DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b667318f-96ad-4b4e-bc6c-0ca5481a6ef4",
   "metadata": {},
   "source": [
    "In the original version of this script, developed for analysis of survey responses, the topic modeling process was done on each column separately and then pd.merge was used to merge them all back to the original for Tableau reporting purposes. That step is unnecessary here but code would be:\n",
    "\n",
    "df_final= pd.merge(\n",
    "\tpd.merge(\n",
    "\t\tpd.merge(\n",
    "\t\t\tdf_clean,\n",
    "\t\t\tdf1, on=['join_key'],\n",
    "\t\t\thow='left'\n",
    "\t    ),\n",
    "\t\tdf2,on=['join_key'],\n",
    "\t\thow='left'\n",
    "\t),\n",
    "    df3,on=['join_key'],\n",
    "\thow='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c9cdd3d-bbc4-41bd-83a8-0640c162180d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('youtube_comments_for_reporting.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0039712-00b4-47af-ba96-580fb4db94fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
